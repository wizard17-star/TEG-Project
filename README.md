# TEG_PROJECT
Project Report: Question Answering System Using RAG (Retrieval-Augmented Generation)
Objective:

The objective of this project is to develop a Question Answering (QA) system using Retrieval-Augmented Generation (RAG) with the Gemini AI model. The goal is to integrate an AI-powered retrieval mechanism to enhance the accuracy and relevance of the answers provided by the model.

1. Data and Dataset
The dataset used for this project consists of a question bank containing various keywords and corresponding questions. This dataset is stored in a CSV file, with columns that include:

Keyword: The main subject or topic of the question.

Question: The question text itself, which needs to be answered by the system.

Example data:

Keyword	Question
England	What is the capital of England?
France	What is the national flower of France?
2. System Overview
The system uses a Retrieval-Augmented Generation (RAG) approach, which combines the power of traditional search-based retrieval with the generation capabilities of deep learning models. The steps involved in the process are as follows:

Step 1: Dataset Preprocessing
Loading Data: The dataset was loaded into a pandas DataFrame for easy manipulation.

Expanding Queries: Each question was expanded using a simple prompt engineering approach to generate multiple variations of the question. This is done to ensure that the AI model can process the questions more effectively.

Step 2: Model Selection and Integration
Model Choice: Gemini AI, which uses advanced generative techniques, was selected for answering questions. The model was chosen due to its advanced language capabilities and suitability for complex QA tasks.

Retrieval-Augmented Generation: In this approach, the system first retrieves relevant information from a dataset and then generates an answer using a language model. This two-step approach ensures that the answers are more accurate, as the model is augmented with specific context before generating a response.

Step 3: Query Expansion and Answer Generation
Prompt Engineering: To improve the generation process, various query expansion techniques were applied. The original question was expanded into multiple forms (e.g., rephrasing or asking about specific aspects of the question).

Answer Generation: The expanded queries were fed into the Gemini AI model, which generated answers based on the given context.

Step 4: Evaluation
Evaluation Metrics: The model’s responses were evaluated based on three key metrics:

Faithfulness: How well does the answer match the context and information provided in the dataset?

Context Recall: Does the answer recall relevant information from the context?

Answer Relevancy: How relevant is the answer to the original question?

These metrics were used to assess the overall performance of the model in providing accurate and relevant answers.

Step 5: Results Visualization
Results: The answers generated by the Gemini AI model were compared against the expected answers. The evaluation results were stored and visualized to track the performance of the model across different questions.

Example results:

Question	Faithfulness	Context Recall	Answer Relevancy
What is the capital of England?	0.9	0.8	1.0
What is the national flower of France?	0.85	0.75	0.95
3. Challenges and Solutions
Data Quality: One of the main challenges was ensuring the data was properly preprocessed. The questions needed to be expanded and refined to make the model’s task easier.

Model Integration: Integrating the Gemini AI model with a custom retrieval system was initially challenging due to API authentication and model setup issues. However, after configuring the credentials and ensuring the correct API access, the model was successfully integrated.

4. Conclusion
In this project, we successfully built a Question Answering system using the Retrieval-Augmented Generation (RAG) approach with Gemini AI. We followed the key steps of data preprocessing, prompt engineering, model integration, and evaluation to achieve accurate and relevant results. The system was able to generate meaningful answers based on expanded queries and retrieve contextual information to enhance its response quality.

5. Future Work
Model Fine-tuning: Future improvements could involve fine-tuning the Gemini model for more specific tasks or domains.

Better Retrieval Mechanism: Using a more advanced retrieval system or integrating external databases could improve the system’s performance further.
